# -*- coding: utf-8 -*-
"""finals.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12-mDpWFJTTck5nng8aZ_fOLHQjOwcR_8
"""



import google.generativeai as genai

genai.configure(api_key="AIzaSyA4p-7iZ8KTnth355WEN3wQmsLLUFZqU6c")
llm = genai.GenerativeModel("gemini-1.5-flash")

from firecrawl import FirecrawlApp

# Initialize FirecrawlApp with your API key
app = FirecrawlApp(api_key="fc-0b3508f981554b2ca1da670d49943f81")

# List of URLs to scrape
urls = [
    'https://nogood.io/2022/11/04/linkedin-ad-examples/',
    'https://online.sbu.edu/news/successful-marketing-campaigns',
    'https://thirdeyeblindproductions.com/10-brand-campaigns-from-india-that-will-inspire-you/',
    'https://www.linkedin.com/business/marketing/blog/linkedin-ads/10-examples-of-linkedin-ads-that-totally-crushed-it',
    'https://www.digitaltripathi.com/ad-library/mcdonalds-every-coffee-is-a-story-ad-campaign/',
    'https://www.digitaltripathi.com/ad-library/kalyan-jewellers-ad-featuring-rashmika-and-kalyani/',
    'https://www.digitaltripathi.com/ad-library/bjp-party-ad-campaign-featuring-rupali-ganguly/',
    'https://www.digitaltripathi.com/ad-library/samsung-newspaper-ad-circle-it-for-galaxy-s24-ultra/ ',
    'https://www.digitaltripathi.com/ad-library/ikea-desi-ooh-billboards-takes-over-indian-cities/',
    'https://www.digitaltripathi.com/ad-library/ustad-zakir-hussain-and-taj-mahal-tea-campaign/'
]

# Loop through the list of URLs and scrape each one
scrape_results = []
for url in urls:
    scrape_result = app.scrape_url(url, params={'formats': ['markdown', 'html']})
    scrape_results.append(scrape_result)

# Print the results
for result in scrape_results:
    print(result)

"""# Helpers"""

def clean_html(html):
    # Remove script and style
    html = re.sub(r'<script.*?>.*?</script>', '', html, flags=re.DOTALL)
    html = re.sub(r'<style.*?>.*?</style>', '', html, flags=re.DOTALL)

    # Remove comments
    html = re.sub(r'<!--.*?-->', '', html, flags=re.DOTALL)

    # Remove hyperlinks (anchor tags but keep text)
    html = re.sub(r'<a\s+.*?>|</a>', '', html, flags=re.DOTALL)

    # Remove HTML tags
    html = re.sub(r'<.*?>', '', html)

    # Remove specific attributes
    html = re.sub(r'\s(?:class|id|style|href)="[^"]*"', '', html)

    # Remove excess whitespace
    html = re.sub(r'\s+', ' ', html)

    return html.strip()

def extract_url(chunk):
    """Extract the first URL found in the given chunk."""
    match = re.search(r'https?://[^\s\)\]]+', chunk)
    return match.group(0) if match else None

def split_into_chunks_with_analysis(text, chunk_size=2000, overlap=200):
    """Split text into chunks with overlap and map each chunk to its URL, ad type, and sentiment."""
    chunks = []
    start = 0
    text_length = len(text)

    while start < text_length:
        # Define the chunk's end index
        end = min(start + chunk_size, text_length)
        chunk = text[start:end]

        # Extract URL from the current chunk
        url = extract_url(chunk)

        # Append the chunk details as a dictionary
        chunks.append({
            "chunk": chunk,
            "url": url,
        })

        # Move the start point forward by chunk_size minus overlap
        start += chunk_size - overlap

    # Convert chunks to JSON
    return json.dumps({"chunks": chunks}, indent=4)

import re
import json
import time
import csv

# Example usage
cleaned_html = clean_html(scrape_result["markdown"])  # Clean the scraped HTML
chunks_json = split_into_chunks_with_analysis(cleaned_html, chunk_size=1000, overlap=200)

# Parse JSON to access chunks if needed
index_page = json.loads(chunks_json)["chunks"]

# Define the output CSV file path
output_file = 'blogs_info.csv'

# Write the JSON data to a CSV file
with open(output_file, 'w', newline='') as f:
    # Create a CSV DictWriter object
    writer = csv.DictWriter(f, fieldnames=["chunk", "url"])

    # Write the header
    writer.writeheader()

    # Write the rows of data
    writer.writerows(index_page)

print(f"Data exported to {output_file} successfully.")